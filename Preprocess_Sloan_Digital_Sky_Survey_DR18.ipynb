{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dansah2/Sloan-Digital-Sky-Survey---DR18/blob/main/Preprocess_Sloan_Digital_Sky_Survey_DR18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hMxaRaUgS57"
      },
      "source": [
        "#Sloan Digital Sky Survey - DR18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmqXtL_Vgbef"
      },
      "source": [
        "This dataset consists of 100,000 observations from the Data Release (DR) 18 of the Sloan Digital Sky Survey (SDSS). Each observation is described by 42 features and 1 class column classifying the observation as either:\n",
        "\n",
        "a STAR\n",
        "a GALAXY\n",
        "a QSO (Quasi-Stellar Object) or a Quasar.\n",
        "\n",
        "Kaggle Dataset Download API Command:\n",
        "\n",
        "kaggle datasets download -d diraf0/sloan-digital-sky-survey-dr18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAto-d2mgoTQ"
      },
      "source": [
        "#Project Outline:\n",
        "1) Download the dataset\n",
        "\n",
        "2) Explore/Analyze the data\n",
        "\n",
        "3) Preprocess and organize the data for ML training\n",
        "\n",
        "4) Set appropriate weights\n",
        "\n",
        "5) Create and Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoMar6Krgpol"
      },
      "source": [
        "##Download / Read the Dataset\n",
        "1) Install required libraries\n",
        "\n",
        "2) Import required libraries\n",
        "\n",
        "3) Download / Read data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLD_kjO7hS4O"
      },
      "source": [
        "###Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfp9GjQihRld",
        "outputId": "edc493e4-4e24-4b90-d888-352f7dc14e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.4/763.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U kaggle\n",
        "!pip install -q -U scikit-learn\n",
        "!pip install -q -U numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYV_Hnw1hhQP"
      },
      "source": [
        "###Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SUK3SoSgnvS"
      },
      "outputs": [],
      "source": [
        "# handeling data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# downloading data\n",
        "from google.colab import drive\n",
        "\n",
        "# label encoding\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMX7vtdffTEV",
        "outputId": "08a24fe8-e08d-4ccb-e4f4-80c1cd4a04fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive to store Kaggle API for future use\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ0-aFjXh4R-"
      },
      "outputs": [],
      "source": [
        "# make a directory for kaggle temporary instance location in Colab\n",
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hONybl1Gh-Tg"
      },
      "outputs": [],
      "source": [
        "# upload json fine to Google drive and copy the temporary location\n",
        "!cp /content/drive/MyDrive/Kaggle_API/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuNYuaqriA1y"
      },
      "outputs": [],
      "source": [
        "# change the file permissions to read/write to the owner only\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LusSbx9ViDl_",
        "outputId": "815d80f0-69da-4419-c4e0-09a803a62067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sloan-digital-sky-survey-dr18.zip to /content\n",
            "\r  0% 0.00/14.7M [00:00<?, ?B/s]\r 34% 5.00M/14.7M [00:00<00:00, 39.1MB/s]\n",
            "\r100% 14.7M/14.7M [00:00<00:00, 92.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download -d diraf0/sloan-digital-sky-survey-dr18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4KWe5H7kiwX",
        "outputId": "dda57b84-9c9d-45e2-df0a-8b7eca4ddd7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sloan-digital-sky-survey-dr18.zip\n",
            "  inflating: SDSS_DR18.csv           \n"
          ]
        }
      ],
      "source": [
        "! unzip sloan-digital-sky-survey-dr18.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdasQl-fkmaL"
      },
      "outputs": [],
      "source": [
        "def read_function(csv_file):\n",
        "    return pd.read_csv(csv_file)\n",
        "\n",
        "raw_data = read_function('SDSS_DR18.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJtdd08-ORCI"
      },
      "source": [
        "##Preprocess and organize the data for ML training\n",
        "1) Label Encoding\n",
        "\n",
        "2) Drop unecessary columns\n",
        "\n",
        "3) Save Preprocessed data to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Label Encoding"
      ],
      "metadata": {
        "id": "xA5BStACmLu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJskWQuZOkNu"
      },
      "outputs": [],
      "source": [
        "# label encoding\n",
        "def encode_label(data_frame, target, new_col_name):\n",
        "  # creating instance of labelencoder\n",
        "  labelencoder = LabelEncoder()\n",
        "\n",
        "  #assign numerical values and store in another column\n",
        "  data_frame[new_col_name] = labelencoder.fit_transform(data_frame[target])\n",
        "\n",
        "  return data_frame\n",
        "\n",
        "raw_data = encode_label(raw_data, 'class', 'e_class')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Drop Unecessary Columns"
      ],
      "metadata": {
        "id": "pkHa_nS8mOxO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM1yP_maYM6v",
        "outputId": "16712113-81d7-4a51-d592-c0fbf11894a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The remaining colums are: Index(['objid', 'specobjid', 'ra', 'dec', 'u', 'g', 'r', 'i', 'z', 'run',\n",
            "       'rerun', 'camcol', 'field', 'plate', 'mjd', 'fiberid', 'petroRad_u',\n",
            "       'petroRad_g', 'petroRad_i', 'petroRad_r', 'petroRad_z', 'petroFlux_u',\n",
            "       'petroFlux_g', 'petroFlux_i', 'petroFlux_r', 'petroFlux_z',\n",
            "       'petroR50_u', 'petroR50_g', 'petroR50_i', 'petroR50_r', 'petroR50_z',\n",
            "       'psfMag_u', 'psfMag_r', 'psfMag_g', 'psfMag_i', 'psfMag_z', 'expAB_u',\n",
            "       'expAB_g', 'expAB_r', 'expAB_i', 'expAB_z', 'redshift', 'e_class'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# drop columns\n",
        "def drop_columns(data_frame, col_name):\n",
        "  try:\n",
        "    data_frame = data_frame.drop(columns=col_name)\n",
        "    print(f'The remaining colums are: {data_frame.columns}')\n",
        "  except:\n",
        "    print(f'The column(s) have already been dropped, the remaining are: {data_frame.columns}')\n",
        "  return data_frame\n",
        "\n",
        "raw_data = drop_columns(raw_data, ['class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Save Preprocessed data to Google Drive"
      ],
      "metadata": {
        "id": "GcFjhR1-mbQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# export the data\n",
        "raw_train.to_csv('/content/drive/My Drive/NAME_OF_FOLDER/train_df.csv', index=False)\n",
        "raw_test.to_csv('/content/drive/My Drive/NAME_OF_FOLDER/test_df.csv', index=False)\n",
        "\n",
        "#verify it was exported\n",
        "df_verify = pd.read_csv('/content/drive/My Drive/NAME_OF_FOLDER/train_df.csv')\n",
        "print(df_verify)"
      ],
      "metadata": {
        "id": "n-mIEX8ZmlGG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFuNzBs/KdPaGBRzTLtFjm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}